# Adversarial Robustness and Explainable AI for CNNs
*On-going Project*

## Project Status
⚠️ Work in progress — only part of the thesis is included; LIME and SHAP for tabular data are under development.

## Introduction

This repository contains a portion of the work from my completed thesis on **adversarial machine learning** and **explainable AI** for convolutional neural networks (CNNs). It focuses on:  
- Evaluating CNN robustness against adversarial attacks.  
- Using **LIME** (Local Interpretable Model-Agnostic Explanations) to interpret CNN predictions under different interpretability settings, both with and without adversarial training.  

This is **only part of the thesis** — not all experiments or analyses are included. Additionally, work is **ongoing** to:  
- Extend interpretability to **SHAP**.  
- Apply both **LIME** and **SHAP** to **tabular data**.
