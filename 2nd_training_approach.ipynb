{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFbniyXSK12O",
    "outputId": "a6a67f20-71f7-4778-e689-2a636d568eec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:40:58.075693: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-09 18:41:10.728440: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-09 18:41:10.777371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-09 18:41:20.193932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook running: Keras 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import keras\n",
    "import skimage.io\n",
    "import skimage.segmentation\n",
    "import copy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "print('Notebook running: Keras', keras.__version__) # Zeigt die Keras Version\n",
    "# print('Notebook running: TF', tensorflow.__version__)\n",
    "from skimage.segmentation import slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5A3-ZVfK12R"
   },
   "outputs": [],
   "source": [
    "# Loadthe dataset\n",
    "training_file = \"train.p\"\n",
    "validation_file = \"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(validation_file, 'rb') as f:\n",
    "    valid_data = pickle.load(f)\n",
    "with open(testing_file, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train_data['features'], train_data['labels']\n",
    "X_valid, y_valid = valid_data['features'], valid_data['labels']\n",
    "X_test, y_test = test_data['features'], test_data['labels']\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_valid = X_valid.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "n_classes = len(set(y_train))\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_valid = to_categorical(y_valid, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "# Load label names\n",
    "label_names = open('signnames.csv').read().strip().split(\"\\n\")[1:]\n",
    "label_names = [line.split(',') for line in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7q8mn89K12T"
   },
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(32, 32, 3)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgmKcuNEK12V"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save initial weights\n",
    "model_no_training = create_cnn_model()\n",
    "model_no_training.save_weights('init_weights_2501_20estop_02.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6AVrAcdK12W"
   },
   "outputs": [],
   "source": [
    "# Function to generate adversarial examples using FGSM on superpixels\n",
    "def generate_fgsm_superpixel(model, image, label, epsilon, segments):\n",
    "    \"\"\"\n",
    "    Generate FGSM adversarial examples targeting superpixels.\n",
    "    Args:\n",
    "    - model: Pretrained model.\n",
    "    - image: Input image.\n",
    "    - label: True label (one-hot encoded).\n",
    "    - epsilon: Perturbation magnitude.\n",
    "    - segments: Superpixel segmentation of the input image.\n",
    "\n",
    "    Returns:\n",
    "    - adversarial_image: Generated adversarial image.\n",
    "    \"\"\"\n",
    "    image = tf.convert_to_tensor(image[np.newaxis, ...], dtype=tf.float32)\n",
    "    label = tf.convert_to_tensor(label[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        predictions = model(image)\n",
    "        loss = CategoricalCrossentropy()(label, predictions)\n",
    "\n",
    "    gradient = tape.gradient(loss, image).numpy()[0]\n",
    "\n",
    "    # Create adversarial image by perturbing each superpixel\n",
    "    adversarial_image = image.numpy()[0]\n",
    "    unique_superpixels = np.unique(segments)\n",
    "    for superpixel_id in unique_superpixels:\n",
    "        mask = (segments == superpixel_id)\n",
    "        avg_gradient = np.sum(gradient[mask], axis=0)  # Average gradient\n",
    "        adversarial_image[mask] += epsilon * np.sign(avg_gradient)  # Perturb the superpixel\n",
    "\n",
    "    # Clip pixel values to valid range [0, 1]\n",
    "    adversarial_image = np.clip(adversarial_image, 0, 1)\n",
    "    return adversarial_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuncoUEoK12X"
   },
   "outputs": [],
   "source": [
    "# Generate adversarial dataset for training\n",
    "def generate_adversarial_dataset_superpixel(model, X_data, y_data, epsilon, superpixel_segments):\n",
    "    \"\"\"\n",
    "    Generate adversarial dataset using FGSM with superpixels.\n",
    "    Args:\n",
    "    - model: Pretrained model.\n",
    "    - X_data: Dataset of images.\n",
    "    - y_data: Dataset of labels (one-hot encoded).\n",
    "    - epsilon: Perturbation magnitude.\n",
    "    - superpixel_segments: Precomputed superpixel segmentations for the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - X_adversarial: Array of adversarial images.\n",
    "    \"\"\"\n",
    "    X_adversarial = []\n",
    "    for i in range(len(X_data)):\n",
    "        adversarial_image = generate_fgsm_superpixel(model, X_data[i], y_data[i], epsilon, superpixel_segments[i])\n",
    "        X_adversarial.append(adversarial_image)\n",
    "    return np.array(X_adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CgPDimYK12Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Precompute superpixels for the dataset\n",
    "def generate_superpixels(X_data, n_segments=50, compactness=5, sigma=1):\n",
    "    \"\"\"\n",
    "    Generate superpixel segmentations for a dataset.\n",
    "    Args:\n",
    "    - X_data: Dataset of images.\n",
    "    - n_segments: Number of superpixels.\n",
    "    - compactness: Compactness parameter for SLIC.\n",
    "    - sigma: Gaussian smoothing for SLIC.\n",
    "\n",
    "    Returns:\n",
    "    - List of segmentations for each image.\n",
    "    \"\"\"\n",
    "    return [slic(X_data[i], n_segments=n_segments, compactness=compactness, sigma=sigma) for i in range(len(X_data))]\n",
    "\n",
    "# Generate superpixels for training and validation datasets\n",
    "train_segments = generate_superpixels(X_train)\n",
    "valid_segments = generate_superpixels(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN8zEIlWK12Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform adversarial training\n",
    "def adversarial_training(model, X_train, y_train, X_valid, y_valid, train_segments, epochs, batch_size, alpha, epsilon, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "        train_segments = [train_segments[i] for i in indices]\n",
    "\n",
    "        for batch_start in range(0, len(X_train), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_train))\n",
    "            x_batch = X_train[batch_start:batch_end]\n",
    "            y_batch = y_train[batch_start:batch_end]\n",
    "            segments_batch = train_segments[batch_start:batch_end]\n",
    "\n",
    "            x_adv_batch = generate_adversarial_dataset_superpixel(model, x_batch, y_batch, epsilon, segments_batch)\n",
    "\n",
    "            # Training step\n",
    "            x_batch = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "            y_batch = tf.convert_to_tensor(y_batch, dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions_clean = model(x_batch)\n",
    "                loss_clean = CategoricalCrossentropy()(y_batch, predictions_clean)\n",
    "\n",
    "                predictions_adv = model(x_adv_batch)\n",
    "                loss_adv = CategoricalCrossentropy()(y_batch, predictions_adv)\n",
    "\n",
    "                combined_loss = alpha * loss_clean + (1 - alpha) * loss_adv\n",
    "\n",
    "            gradients = tape.gradient(combined_loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        val_loss, val_acc = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7R8p11iK12b"
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "alpha = 0.5\n",
    "epsilon = 0.01\n",
    "patience = 20  # Early stopping patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVzSmVkuK12c",
    "outputId": "71c98ab2-9d79-4681-cd03-5450ad8c6430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Validation Loss: 0.7488, Validation Accuracy: 0.8070\n",
      "Epoch 2/200\n",
      "Validation Loss: 0.4697, Validation Accuracy: 0.8921\n",
      "Epoch 3/200\n",
      "Validation Loss: 0.4753, Validation Accuracy: 0.8950\n",
      "Epoch 4/200\n",
      "Validation Loss: 0.3594, Validation Accuracy: 0.9234\n",
      "Epoch 5/200\n",
      "Validation Loss: 0.3871, Validation Accuracy: 0.9202\n",
      "Epoch 6/200\n",
      "Validation Loss: 0.5277, Validation Accuracy: 0.9075\n",
      "Epoch 7/200\n",
      "Validation Loss: 0.4586, Validation Accuracy: 0.9209\n",
      "Epoch 8/200\n",
      "Validation Loss: 0.3254, Validation Accuracy: 0.9356\n",
      "Epoch 9/200\n",
      "Validation Loss: 0.3382, Validation Accuracy: 0.9420\n",
      "Epoch 10/200\n",
      "Validation Loss: 0.5286, Validation Accuracy: 0.9272\n",
      "Epoch 11/200\n",
      "Validation Loss: 0.4425, Validation Accuracy: 0.9474\n",
      "Epoch 12/200\n",
      "Validation Loss: 0.3748, Validation Accuracy: 0.9388\n",
      "Epoch 13/200\n",
      "Validation Loss: 0.2601, Validation Accuracy: 0.9537\n",
      "Epoch 14/200\n",
      "Validation Loss: 0.2969, Validation Accuracy: 0.9483\n",
      "Epoch 15/200\n",
      "Validation Loss: 0.3607, Validation Accuracy: 0.9422\n",
      "Epoch 16/200\n",
      "Validation Loss: 0.3070, Validation Accuracy: 0.9528\n",
      "Epoch 17/200\n",
      "Validation Loss: 0.4385, Validation Accuracy: 0.9531\n",
      "Epoch 18/200\n",
      "Validation Loss: 0.3753, Validation Accuracy: 0.9449\n",
      "Epoch 19/200\n",
      "Validation Loss: 0.2224, Validation Accuracy: 0.9583\n",
      "Epoch 20/200\n",
      "Validation Loss: 0.4189, Validation Accuracy: 0.9503\n",
      "Epoch 21/200\n",
      "Validation Loss: 0.3738, Validation Accuracy: 0.9590\n",
      "Epoch 22/200\n",
      "Validation Loss: 0.3758, Validation Accuracy: 0.9624\n",
      "Epoch 23/200\n",
      "Validation Loss: 0.3236, Validation Accuracy: 0.9621\n",
      "Epoch 24/200\n",
      "Validation Loss: 0.3293, Validation Accuracy: 0.9449\n",
      "Epoch 25/200\n",
      "Validation Loss: 0.2423, Validation Accuracy: 0.9639\n",
      "Epoch 26/200\n",
      "Validation Loss: 0.3243, Validation Accuracy: 0.9551\n",
      "Epoch 27/200\n",
      "Validation Loss: 0.3775, Validation Accuracy: 0.9610\n",
      "Epoch 28/200\n",
      "Validation Loss: 0.3303, Validation Accuracy: 0.9485\n",
      "Epoch 29/200\n",
      "Validation Loss: 0.4492, Validation Accuracy: 0.9503\n",
      "Epoch 30/200\n",
      "Validation Loss: 0.3526, Validation Accuracy: 0.9533\n",
      "Epoch 31/200\n",
      "Validation Loss: 0.5116, Validation Accuracy: 0.9576\n",
      "Epoch 32/200\n",
      "Validation Loss: 0.5367, Validation Accuracy: 0.9635\n",
      "Epoch 33/200\n",
      "Validation Loss: 0.5034, Validation Accuracy: 0.9590\n",
      "Epoch 34/200\n",
      "Validation Loss: 0.5189, Validation Accuracy: 0.9612\n",
      "Epoch 35/200\n",
      "Validation Loss: 0.4257, Validation Accuracy: 0.9639\n",
      "Epoch 36/200\n",
      "Validation Loss: 0.4758, Validation Accuracy: 0.9633\n",
      "Epoch 37/200\n",
      "Validation Loss: 0.3747, Validation Accuracy: 0.9580\n",
      "Epoch 38/200\n",
      "Validation Loss: 0.4319, Validation Accuracy: 0.9637\n",
      "Epoch 39/200\n",
      "Validation Loss: 0.3549, Validation Accuracy: 0.9660\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/le/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Train adversarial model\n",
    "model_adv_training = create_cnn_model()\n",
    "model_adv_training = adversarial_training(model_adv_training, X_train, y_train, X_valid, y_valid, train_segments, epochs, batch_size, alpha, epsilon, patience)\n",
    "\n",
    "# # Save the adversarially trained model\n",
    "model_adv_training.save(\"CNN_adv_training_paper_superpixel_estop20_2501_02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRnqNghaK12d",
    "outputId": "15c9f581-05b6-492c-f880-60921c0a5e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 2s 5ms/step - loss: 0.4232 - accuracy: 0.9512\n",
      "395/395 [==============================] - 2s 5ms/step - loss: 0.8879 - accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on clean and adversarial test data\n",
    "test_segments = generate_superpixels(X_test)\n",
    "X_test_adv = generate_adversarial_dataset_superpixel(model_adv_training, X_test, y_test, epsilon, test_segments)\n",
    "\n",
    "test_loss, test_acc = model_adv_training.evaluate(X_test, y_test)\n",
    "adv_test_loss, adv_test_acc = model_adv_training.evaluate(X_test_adv, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
